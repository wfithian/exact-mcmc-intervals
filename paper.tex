\documentclass{article}


\usepackage[round]{natbib}
\usepackage{amsmath,amssymb,amsthm,bm,enumerate,mathrsfs,mathtools}
\usepackage{latexsym,color,verbatim,multirow}
\usepackage[margin=1.3in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\tikzstyle{block} = [rectangle, draw, fill=white!20,
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{title} = [text width=7em, text centered, font=\bfseries]
\tikzstyle{line} = [draw, -latex']

\usepackage{mycommands}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{model}[theorem]{Model}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{example}{Example}

\newcommand{\cM}{\mathcal{M}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\FDR}{\textnormal{FDR}}
\newcommand{\FCR}{\textnormal{FCR}}
\newcommand{\crt}{\phi}
\newcommand{\M}{\mathcal{M}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\bX}{X}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\hcQ}{\widehat{\mathcal{Q}}}
\newcommand{\bbeta}{{\bm\beta}}
\newcommand{\beps}{{\bm\epsilon}}
\newcommand{\bmu}{{\bm\mu}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Gv}{\;\;\big|\;\;}
\newcommand{\proj}{\cP}
\newcommand{\pow}{\text{Pow}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\msF}{m\mathscr{F}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\hM}{\widehat{M}}
\newcommand{\hI}{\widehat{I}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\leqAS}{\overset{\textrm{a.s.}}{\leq}}
\newcommand{\cL}{\mathcal{L}}

\newcommand*\mystrut{\vrule width0pt height0pt depth1.5ex\relax}
\newcommand{\underlabel}{\underbracket[1pt][.5pt]{\mystrut \quad\;\; \sub \quad\;\; }}
\newcommand{\WFcomment}[1]{{\color{red}{(WF: \bf \sc #1) }}}
\newcommand{\paraOrSubsub}{\subsubsection}
\newcommand{\sampOrData}{data }
\newcommand{\capSampOrData}{Data }


\begin{document}



\title{Exact Confidence Intervals via Markov Chain Monte Carlo}
\maketitle


\begin{abstract}
  \citet{besag1989generalized} propose two ingenious methods for testing a null hypothesis when we cannot sample from the null distribution, but we can construct a Markov chain whose stationary distribution is the null distribution. Both of their tests are exact level $\alpha$ even if the chain does not mix well. We show here how to generalize this method to obtain confidence intervals. We discuss applications to selective inference. 
\end{abstract}


\section{Introduction}

Consider data $X\in \cX$, arising from the model $X \sim \pi_\theta(x)$, with $\theta\in\R$. Based on the ideas of \citet{besag1989generalized}, if I can sample from $\pi_0$ via MCMC then I can test $H_0:\;\theta=0$. They propose two methods: the {\em parallel method}, which we address in Section~\ref{sec:parallel}, and the {\em serial method}, which we do not address. For simplicity we assume throughout that the state space $\cX$ is discrete.

We consider here how to modify this method to construct a valid {\em confidence interval} for $\theta$. We would like to directly invert the test of \citet{besag1989generalized}, but that would involve running a chain with stationary distribution $\pi_{\theta_0}$ for every $\theta_0\in \R$: obviously an impractical solution. Can we instead re-use the data coming from the $\theta=0$ chain to test $H_0:\;\theta=\theta_0$, for every $\theta_0\in\R$?

There are two hurdles to clear to obtain a confidence interval. First, we must be able to carry out an exact test of $H_0:\; \theta=\theta_0$ using a chain whose stationary distribution corresponds to $\theta=0$.

Second, we must somehow {\em couple} the tests of different $\theta_0\in \R$ so that inverting them actually yields an {\em integral}: that is, we hope to find $\theta_1\leq \theta_2$ such that every $\theta_0<\theta_1$ is rejected, as well as every $\theta_0>\theta_2$. Ideally, we would also have every $\theta_0\in [\theta_1,\theta_2]$ accepted, but this is not absolutely essential: as long as the confidence region $K=\{\theta_0:\; \theta=\theta_0 \text{ not rejected }\}$ is bounded, we can always return the interval $[\inf K, \sup K]$.

\section{Modifying the Parallel Method}\label{sec:parallel}

In the {\em parallel method}, we run the chain backward one step from $X$ to $Y$, and then run the chain forward one step from $Y$, in parallel, $B$ times to obtain $X_2,\ldots, X_n$. If $X$ is a sample from the stationary distribution of the Markov chain (as it is under the null hypothesis), then $(X,X_2,\ldots,X_n)$ is exchangeable. If $T(X)$ is any test statistic and $T_i=T(X_i)$, then the rank of $T(X)$ among $(T(X), T_2, \ldots, T_n)$  is therefore uniform under the null hypothesis that $\theta=0$.

Assume that $P_0(x,y)$ is a Markov chain with stationary distribution $\pi_0$, and assume that $Q_0(x,y)$ is a valid ``backward step'' in the sense that 
\[
\pi_0(x)Q(x,y) = \pi_0(y)P(y,x)
\]

Let $\P_0$ denote the probability measure with $X\sim \pi_0$. Ten  $(X,X_2,\ldots,X_n)$ are exchangeable under the null because
\begin{align}
  \P_0(X,Y,X_{2:n})
  &= \pi_0(X)Q_0(X,Y) \prod_{i=2}^n P_0(Y,X_i)\\
  &= \pi_0(Y)P_0(Y,X) \prod_{i=2}^n P_0(Y,X_i),
\end{align}
which is plainly invariant to permuting $(X,X_2,\ldots,X_n)$. Moreover, $(X,X_2,\ldots,X_n)$ are actually conditionally independent given $Y$.

If $X\sim \pi_1$ instead, then the distribution is {\em not} exchangeable, since
\[
  \P_1(X, \,Y, \,X_{2:n})
  = \frac{\pi_1(X)}{\pi_0(X)} \cdot \pi_0(Y)P_0(Y,X) \prod_{i=2}^n P_0(Y,X_i).
\]

\subsection{Accept/Reject Sampling}

We can, however, use accept/reject sampling to achieve exchangeability among the accepted sample. Assume $c=\sup_{x\in\cX} \frac{\pi_1(x)}{\pi_0(x)} \leq \infty$.

Then, taking $U_i \simiid U[0,1], i=2:n$ independently of the chain, let
\[
Z_i = \1\left\{U_i \leq \frac{\pi_1(X_i)}{c\pi_0(X_i)}\right\}.
\]
Finally, let $S=\{i: Z_i = 1\}$.

The augmented likelihood of $(X,Y,X_{2:n}, Z_{2:n})$ is
\begin{align}
  \P_0(X,Y,X_{2:n},Z_{2:n}) 
  &= \pi_1(X)Q_0(X,Y) \;\prod_{i\in S} P_0(Y,X_i)\frac{\pi_1(X_i)}{c\pi_0(X_i)}
  \; \prod_{i\in S^C} P_0(Y,X_i)\left(1-\frac{\pi_1(X_i)}{c\pi_0(X_i)}\right)\\
  &= c^{-|S|}\pi_0(Y)P_0(Y,X)\frac{\pi_1(X)}{\pi_0(X)} \;\prod_{i\in S} P_0(Y,X_i)\frac{\pi_1(X_i)}{\pi_0(X_i)}
  \; \prod_{i\in S^C} P_0(Y,X_i)\left(1-\frac{\pi_1(X_i)}{c\pi_0(X_i)}\right).
\end{align}
Conditioning on the set $S=\{i_1,\ldots,i_{|S|}\}$ of accepted indices, the distribution of $(X, X_{i_1},\ldots,X_{i_{|S|}})$ is once again exchangeable.

Note that this may not be a very good practical strategy if $c$ is huge. It may be advantageous to construct the chain with respect to a distribution that has somewhat heavier tails than the distribution for any $\theta_0$ (really, there is no requirement that $\pi_0$ should necessarily come from our model).

\subsection{Constructing an Interval}

Next we consider how to actually construct a confidence interval for $\theta$ once we know how to test any fixed hypothesis $H_0:\; \theta=\theta_0$ given the same sample. We assume that our model satisfies monotone likelihood ratio with respect to some test statistic $T(X)$. This assumption guarantees that a right-tailed test of $H_0:\; \theta=\theta_0$, based on $T(X)$, is conservative under $\theta\leq \theta_0$. That is, our procedure for testing $H_0:\; \theta=\theta_0$ is equivalent to testing $H_0:\; \theta\leq \theta_0$ when we use $T(X)$ as our test statistic. \WFcomment{show this also works for the conditional testing setup here}.

To construct an exact (conservative) interval, we will first lay down a grid of $\theta$ values, $\theta_1 \leq \cdots \leq \theta_T$. For each $t=1,\ldots,T$, we will construct $p$-values $p_t^{\pm}$ for the hypotheses 
\[
H_t^-:\; \theta\leq \theta_t \quad \text{ and } \quad 
H_t^+:\; \theta\geq \theta_t.
\]

By construction, the $p$-values will satisfy
\[
\P_\theta\left( p_t^- \leq \alpha) \leq \alpha, \quad \forall \theta\leq \theta_t.
\]
To make the $p$-values for different values of $t$ as correlated as possible, we will use the same $U_i$ for every $t$. We will not require any assumptions about independence of the $p$-values for different $t$, so this coupling does not affect the validity of our procedure in any way.

For notational convenience, we take $\theta_1=-\infty$ and $\theta_T=+\infty$, with $p_1^-=p_T^+=0$ and $p_1^+=p_T^-=1$.

To obtain the upper and lower confidence bounds we set
\[
C^- = \min\{\theta_t:\; p_{t+1}^- > \alpha/2\}, \quad 
C^+ = \max\{\theta_t:\; p_{t-1}^+ > \alpha/2\},
\]

\begin{proposition}
  $[C^-, C^+]$ as defined above is a conservative level-$\alpha$ confidence interval for $\theta$.
\end{proposition}
\begin{proof}
  Let $t^-(\theta) = \min\{t:\; \theta < \theta_t\}$ and $t^+(\theta)=\max \{t:\; \theta > \theta_t\}$. Then,
\begin{align}
  \P_\theta( \theta \notin [C^-, C^+]) 
  &\leq \P_\theta( C^- > \theta_{t^- - 1}) + \P_\theta( C^+ < \theta_{t^+ + 1})\\
  &\leq \P_\theta( p_{t^-}^- \leq \alpha/2) + \P_\theta( p_{t^+}^+ \leq \alpha/2)\\
  &\leq \alpha/2 + \alpha/2 = \alpha
\end{align}
\end{proof}




\bibliographystyle{plainnat}

\bibliography{biblio}



\end{document}
